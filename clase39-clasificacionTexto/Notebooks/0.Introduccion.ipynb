{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referencias: \n",
    "\n",
    "https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05\n",
    "\n",
    "https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730#.vivglhmhv\n",
    "\n",
    "---\n",
    "\n",
    "A nivel de documento, una de las formas m√°s √∫tiles de entender el texto es analizando sus temas/t√≥picos. \n",
    "\n",
    "El proceso de aprendizaje, reconocimiento y extracci√≥n de estos temas en una colecci√≥n de documentos se denomina modelado de temas (topic modeling).\n",
    "\n",
    "En estas gu√≠as exploraremos el modelado de temas a trav√©s de LSA y LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los algoritmos de topic modelingse basan en el mismo supuesto b√°sico:\n",
    "* Cada documento consta de una mezcla de temas y\n",
    "* Cada tema consiste en una colecci√≥n de palabras.\n",
    "\n",
    "En otras palabras, los modelos de temas se basan en la idea de que la sem√°ntica de nuestro documento en realidad se rige por algunas variables ocultas o \"latentes\" que no estamos observando. \n",
    "\n",
    "Como resultado, el objetivo de topic modeling es descubrir estas variables latentes (temas) que dan forma al significado de nuestro documento y corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA\n",
    "Latent Semantic Analysis, o LSA, es una de las t√©cnicas fundamentales en el modelado de temas. \n",
    "\n",
    "La idea central es tomar una matriz de lo que tenemos (documentos y t√©rminos) y descomponerla en una matriz separada de documentos y temas y una matriz de temas y t√©rminos.\n",
    "\n",
    "El primer paso es generar nuestra matriz documento-t√©rmino. \n",
    "\n",
    "Dados m documentos y n palabras en nuestro vocabulario, podemos construir una matriz A de tama√±o m √ó n en la que cada fila representa un documento y cada columna representa una palabra. \n",
    "\n",
    "En la versi√≥n m√°s simple de LSA, cada entrada puede ser simplemente un recuento sin procesar del n√∫mero de veces que la j-√©sima palabra apareci√≥ en el i-√©simo documento. \n",
    "\n",
    "En la pr√°ctica, sin embargo, los recuentos sin procesar no funcionan particularmente bien porque no tienen en cuenta la importancia de cada palabra en el documento. Por ejemplo, la palabra \"nuclear\" probablemente nos informa m√°s sobre el tema(s) de un documento dado que la palabra \"prueba\".\n",
    "\n",
    "En consecuencia, los modelos LSA generalmente reemplazan los recuentos sin procesar en la matriz de t√©rminos del documento con una puntuaci√≥n tf-idf. \n",
    "\n",
    "Tf-idf, o t√©rmino frecuencia de documento de frecuencia inversa, asigna un peso para el t√©rmino j en el documento i de la siguiente manera:\n",
    "\n",
    "<img src=\"img/tf_idf.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Intuitivamente, un t√©rmino tiene un gran peso cuando aparece con frecuencia en todo el documento pero con poca frecuencia en todo el corpus. \n",
    "\n",
    "La palabra \"compilar\" puede aparecer a menudo en un documento, pero debido a que es probable que sea bastante com√∫n en el resto del corpus, no tendr√° una alta puntuaci√≥n tf-idf. \n",
    "\n",
    "Sin embargo, si la palabra \"gentrificaci√≥n\" aparece a menudo en un documento, porque es m√°s rara en el resto del corpus, tendr√° un puntaje tf-idf m√°s alto.\n",
    "\n",
    "Una vez que tenemos nuestra matriz de t√©rminos de documentos A, podemos comenzar a pensar en nuestros temas latentes. \n",
    "\n",
    "**Con toda probabilidad, A es muy dispersa (sparse), muy ruidosa y muy redundante en sus muchas dimensiones.**\n",
    "\n",
    "Como resultado, para encontrar los pocos temas latentes que capturan las relaciones entre las palabras y los documentos, queremos realizar una reducci√≥n de dimensionalidad en A.\n",
    "\n",
    "Esta reducci√≥n de dimensionalidad se puede realizar usando SVD truncada. \n",
    "\n",
    "SVD, o descomposici√≥n de valores singulares, es una t√©cnica en √°lgebra lineal que factoriza cualquier matriz M en el producto de 3 matrices separadas: M = U * S * V, donde S es una matriz diagonal de los valores singulares de M. \n",
    "\n",
    "<img src=\"img/svd.gif\" alt=\"drawing\" width=\"150\"/>\n",
    "\n",
    "Truncated SVD reduce la dimensionalidad seleccionando solo los t valores singulares m√°s grandes y solo manteniendo las primeras t columnas de U y V. En este caso, t es un hiperpar√°metro que podemos seleccionar y ajustar para reflejar el n√∫mero de temas que queremos encontrar.\n",
    "\n",
    "<img src=\"img/svd_wikipedia.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "\n",
    "Podemos pensarlo como s√≥lo mantener las dimensiones m√°s significativas en nuestro espacio transformado.\n",
    "\n",
    "<img src=\"img/svd_graph.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "\n",
    "En este caso, U de tama√±o (m ‚®â t) emerge como nuestra matriz que relaciona temas y documentos, y V de tama√±o (n ‚®â t) se convierte en nuestra matriz dque relaciona temas y t√©rminos. \n",
    "\n",
    "Tanto en U como en V, las columnas corresponden a uno de nuestros temas t. \n",
    "\n",
    "En U, las filas representan vectores de documentos expresados en t√©rminos de temas; en V, las filas representan vectores de t√©rminos expresados en t√©rminos de temas.\n",
    "\n",
    "Con estos vectores de documentos y vectores de t√©rminos, ahora podemos aplicar f√°cilmente medidas como la similitud de coseno para evaluar:\n",
    "* la similitud de diferentes documentos\n",
    "* la similitud de diferentes palabras\n",
    "* la similitud de t√©rminos (o \"consultas\") y documentos (que se vuelve √∫til en la recuperaci√≥n de informaci√≥n, cuando queremos recuperar los pasajes m√°s relevantes en una b√∫squeda).\n",
    "\n",
    "LSA es r√°pido y eficiente de usar, pero tiene algunos problemas:\n",
    "* resultados poco interpretables (no sabemos cu√°les son los temas, y los componentes pueden ser arbitrariamente positivos / negativos)\n",
    "* Necesidad de un gran conjunto de documentos y vocabulario para obtener resultados precisos\n",
    "* representaci√≥n poco eficiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n",
    "\n",
    "Podemos pensar en dirichlet como una \"distribuci√≥n sobre distribuciones\". \n",
    "\n",
    "En esencia, responde a la pregunta: \"dado este tipo de distribuci√≥n, ¬øcu√°les son algunas de las distribuciones de probabilidad reales que es probable que vea?\"\n",
    "\n",
    "Considere el ejemplo de comparar distribuciones de probabilidad de mezclas de temas. \n",
    "\n",
    "Digamos que el corpus que estamos viendo tiene documentos de 3 √°reas tem√°ticas muy diferentes. \n",
    "\n",
    "Si queremos modelar esto, el tipo de distribuci√≥n que queremos ser√° uno que pese mucho un tema espec√≠fico, y no le d√© mucho peso al resto. \n",
    "\n",
    "Si tenemos 3 temas, entonces algunas distribuciones de probabilidad espec√≠ficas que probablemente ver√≠amos son:\n",
    "Mezcla X: 90% tema A, 5% tema B, 5% tema C\n",
    "Mezcla Y: 5% tema A, 90% tema B, 5% tema C\n",
    "Mezcla Z: 5% tema A, 5% tema B, 90% tema C\n",
    "\n",
    "Si dibujamos una distribuci√≥n de probabilidad aleatoria de esta distribuci√≥n de dirichlet, parametrizada por grandes pesos en un solo tema, probablemente obtendr√≠amos una distribuci√≥n que se parece mucho a la mezcla X, la mezcla Y o la mezcla Z. \n",
    "\n",
    "Ser√≠a muy improbable tener una instancia con distribuci√≥n que es el 33% del tema A, el 33% del tema B y el 33% del tema C.\n",
    "\n",
    "Eso es esencialmente lo que proporciona una distribuci√≥n de dirichlet: una forma de muestreo de distribuciones de probabilidad de un tipo espec√≠fico. \n",
    "\n",
    "<img src=\"img/LDA.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "\n",
    "A partir de una distribuci√≥n de dirichlet Dir (Œ±), extraemos una muestra aleatoria que representa la distribuci√≥n del tema, o mezcla de temas, de un documento en particular. \n",
    "\n",
    "La distribuci√≥n de este tema es Œ∏. \n",
    "\n",
    "Basados en Œ∏, seleccionamos un tema particular Z basado en la distribuci√≥n.\n",
    "\n",
    "A continuaci√≥n, de otra distribuci√≥n de dirichlet Dir (ùõΩ), seleccionamos una muestra aleatoria que representa la distribuci√≥n de palabras del tema Z. \n",
    "\n",
    "Esta distribuci√≥n de palabras es œÜ. De œÜ, elegimos la palabra w.\n",
    "\n",
    "Formalmente, el proceso para generar cada palabra de un documento es el siguiente (tenga en cuenta que este algoritmo usa c en lugar de z para representar el tema):\n",
    "\n",
    "<img src=\"img/LDA_algorithm.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "En LDA, el conjunto de datos sirve como datos de entrenamiento para la distribuci√≥n de dirichlet de distribuciones de temas de documentos. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF\n",
    "\n",
    "LDA se basa en modelos probabil√≠sticos, mientras que NMF se basa en √°lgebra lineal. \n",
    "\n",
    "Ambos algoritmos toman como entrada una matriz bag of words (es decir, cada documento representado como una fila, con cada columna que contiene el recuento de palabras en el corpus). \n",
    "\n",
    "El objetivo de ambos algoritmos es producir 2 matrices m√°s peque√±as; una matriz de documento a tema y una matriz de palabra a tema que cuando se multiplican juntas reproducen la matriz de bolsa de palabras disminuyendo el error.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
