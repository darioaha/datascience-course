{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRACTICA GUIADA 1: Preprocesamiento de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook presentamos NLTK (Natural Language Toolkit) para resolver problemas de procesamiento de lenguaje natural (NLP).\n",
    "\n",
    "https://www.nltk.org/\n",
    "\n",
    "\n",
    "Documentación: https://www.nltk.org/py-modindex.html\n",
    "\n",
    "Libro: http://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tareas básicas de NLP con NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/DS-DH/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data] Downloading package genesis to /home/DS-DH/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/genesis.zip.\n",
      "[nltk_data] Downloading package inaugural to /home/DS-DH/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data] Downloading package nps_chat to /home/DS-DH/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data] Downloading package webtext to /home/DS-DH/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/webtext.zip.\n",
      "[nltk_data] Downloading package treebank to /home/DS-DH/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descargamos algunos ejemplos introductorios:\n",
    "\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('genesis')\n",
    "nltk.download('inaugural')\n",
    "nltk.download('nps_chat')\n",
    "nltk.download('webtext')\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "# Importamos los ejemplos introductorios:\n",
    "\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contando vocabularios de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Wall Street Journal>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sents() devuelve una lista de oraciones o expresiones, cada una codificada como una lista de palabras (string) \n",
    "\n",
    "el resultado es list(list(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent1: Call me Ishmael .\n",
      "sent2: The family of Dashwood had long been settled in Sussex .\n",
      "sent3: In the beginning God created the heaven and the earth .\n",
      "sent4: Fellow - Citizens of the Senate and of the House of Representatives :\n",
      "sent5: I have a problem with people PMing me to lol JOIN\n",
      "sent6: SCENE 1 : [ wind ] [ clop clop clop ] KING ARTHUR : Whoa there !\n",
      "sent7: Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\n",
      "sent8: 25 SEXY MALE , seeks attrac older single lady , for discreet encounters .\n",
      "sent9: THE suburb of Saffron Park lay on the sunset side of London , as red and ragged as a cloud of sunset .\n"
     ]
    }
   ],
   "source": [
    "sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "sent7 devuelve la primera oración del texto 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "len(sent7) devuelve la cantidad de palabras que tiene la primera linea del texto 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "len(text7) devuelve la cantidad de palabras que tiene texto 7 (con repeticiones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100676"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cuento \"a mano\" cuantas palabras hay:\n",
    "tmp = [w for w in text7]\n",
    "len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100676"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "len(set(text7)) devuelve la cantidad de palabras de text7 sin repeticiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12408"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cuento a mano cuantas palabras hay sin repeticiones:\n",
    "tmp = [w for w in text7]\n",
    "tmp_sin_rep = set(tmp)\n",
    "len(tmp_sin_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12408"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(text7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list(set(text7))[:10] devuelve las primeras 10 palabras de text7 sin repeticiones y sin orden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actual',\n",
       " 'senses',\n",
       " 'luxury',\n",
       " 'side',\n",
       " 'fleet',\n",
       " '*T*-161',\n",
       " 'facial',\n",
       " 'producers',\n",
       " '*-120',\n",
       " 'center']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(text7))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frecuencia de las palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 4885, 'the': 4045, '.': 3828, 'of': 2319, 'to': 2164, 'a': 1878, 'in': 1572, 'and': 1511, '*-1': 1123, '0': 1099, ...})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = nltk.FreqDist(text7)\n",
    "\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre', 'Vinken', ',', '61', 'years', 'old', 'will', 'join', 'the', 'board']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab1 = dist.keys()\n",
    "#print(vocab1)\n",
    "#En Python 3 dict.keys() devuelve un objeto iterable\n",
    "list(vocab1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4045"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Podemos observar la cantidad de veces que aparece una palabra en un texto:\n",
    "\n",
    "dist['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['billion',\n",
       " 'company',\n",
       " 'president',\n",
       " 'because',\n",
       " 'market',\n",
       " 'million',\n",
       " 'shares',\n",
       " 'trading',\n",
       " 'program']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos una lista de las palabras de más de 5 caracteres que aparecen más de 100 veces:\n",
    "\n",
    "freqwords = [w for w in vocab1 if len(w) > 5 and dist[w] > 100]\n",
    "freqwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming es un método para reducir una palabra a su raíz o (en inglés) a un stem. \n",
    "\n",
    "Ejemplo en castellano: \"gato\", \"gata\", \"gatos\" => \"gat\"\n",
    "\n",
    "Ejemplo en ingles: \"automates\", \"automatic\"   => \"automat\"\n",
    "\n",
    "El stemming o lematización es hallar el lema (stem) de las palabras y no tiene que tener significado.\n",
    "* Un algoritmo de stemming es un proceso de normalización lingüística en el cual las diferentes formas que puede adoptar\n",
    "una palabra son reducidos a una única forma común, a la cual se denomina stem o lema.\n",
    "* Stem o lema es la porción de la palabra después de eliminar sus afijos. Por ejemplo: perr para las palabras perros, perros,\n",
    "perrito, perrote, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el idioma inglés, puede elegir entre **PorterStammer** o **LancasterStammer**, siendo PorterStemmer el más antiguo desarrollado originalmente en 1979. LancasterStemmer se desarrolló en 1990 y utiliza un enfoque más agresivo que el algoritmo de stemming de Porter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos los algoritmos de Stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer\n",
      "cat\n",
      "troubl\n",
      "troubl\n",
      "troubl\n",
      "\n",
      "\n",
      "Lancaster Stemmer\n",
      "cat\n",
      "troubl\n",
      "troubl\n",
      "troubl\n"
     ]
    }
   ],
   "source": [
    "# Creamos objects a partir de las clases PorterStemmer y LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "\n",
    "\n",
    "# Le pasamos palabras a ambos algoritmos para que hagan stemming:\n",
    "print(\"Porter Stemmer\")\n",
    "print(porter.stem(\"cats\"))\n",
    "print(porter.stem(\"trouble\"))\n",
    "print(porter.stem(\"troubling\"))\n",
    "print(porter.stem(\"troubled\"))\n",
    "print(\"\\n\")\n",
    "print(\"Lancaster Stemmer\")\n",
    "print(lancaster.stem(\"cats\"))\n",
    "print(lancaster.stem(\"trouble\"))\n",
    "print(lancaster.stem(\"troubling\"))\n",
    "print(lancaster.stem(\"troubled\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      lancaster Stemmer   \n",
      "friend              friend              friend              \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "stabil              stabil              stabl               \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n"
     ]
    }
   ],
   "source": [
    "# Creamos una lista de palabras para hacer stemming con ambos algoritmos:\n",
    "\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "# print de titulo:\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "# print de datos:\n",
    "for word in word_list:\n",
    "    porter_stem = porter.stem(word)\n",
    "    lancaster_stem = lancaster.stem(word)\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word, porter_stem, lancaster_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cómo se puede observar el algoritmo de Porter toma a toda la frase como una sola unidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n",
    "porter.stem(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo tanto antes de aplicar stemming tenemos que tokenizar la frase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Es el proceso de separar (y posiblemente clasificar) partes de un string. Puede considerarse una sub tarea del proceso de parsing. Los token resultados son la entrada de otro paso de procesamiento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pythoners',\n",
       " 'are',\n",
       " 'very',\n",
       " 'intelligent',\n",
       " 'and',\n",
       " 'work',\n",
       " 'very',\n",
       " 'pythonly',\n",
       " 'and',\n",
       " 'now',\n",
       " 'they',\n",
       " 'are',\n",
       " 'pythoning',\n",
       " 'their',\n",
       " 'way',\n",
       " 'to',\n",
       " 'success.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tokenizar haciendo un split en los espacios no genera los resultados esperados, porque el punto queda tokenizado junto con la palabra \"success\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Children', \"shouldn't\", 'drink', 'a', 'sugary', 'drink', 'before', 'bed.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Otro ejemplo para ver como falla la tokenización con el split(' ')\n",
    "\n",
    "sentence2 = \"Children shouldn't drink a sugary drink before bed.\"\n",
    "sentence2.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "https://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "sent_tokenize: tokenization en oraciones\n",
    "\n",
    "word_tokenize: tokenization en palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Children',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'drink',\n",
       " 'a',\n",
       " 'sugary',\n",
       " 'drink',\n",
       " 'before',\n",
       " 'bed',\n",
       " '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk.word_tokenize(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is the first sentence.', 'A gallon of milk in the U.S. costs $2.99.', 'Is this the third sentence?', 'Yes, it is!']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is!\"\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(sentences)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first sentence.',\n",
       " 'A gallon of milk in the U.S. costs $2.99.',\n",
       " 'Is this the third sentence?',\n",
       " 'Yes, it is!']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python are veri intellig and work veri pythonli and now they are python their way to success . \n"
     ]
    }
   ],
   "source": [
    "# Definimos una función que tokeniza una frase y aplica el Stemming de Porter:\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    \n",
    "    token_words = word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence = []\n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    \n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "\n",
    "x = stemSentence(sentence)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemmers en otros idiomas:\n",
    "\n",
    "Python **nltk** no solo proporciona dos stemmers en inglés: PorterStemmer y LancasterStemmer, sino también muchos stemmers que no están en inglés como **SnowballStemmers**. (https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.snowball)\n",
    "\n",
    "Idiomas que maneja **SnowballStemmers**:\n",
    "\n",
    " - danés\n",
    " - holandés\n",
    " - inglés\n",
    " - francés\n",
    " - alemán\n",
    " - húngaro\n",
    " - italiano\n",
    " - noruego\n",
    " - portugués\n",
    " - rumano\n",
    " - ruso\n",
    " - español\n",
    " - sueco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "englishStemmer = SnowballStemmer(\"english\")\n",
    "englishStemmer.stem(\"having\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corr'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanishStemmer = SnowballStemmer(\"spanish\")\n",
    "spanishStemmer.stem(\"Corriendo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defimimos una función que aplica stemming a una frase en castellano:\n",
    "\n",
    "def stemfraseesp(frase):\n",
    "    \n",
    "    token_words = word_tokenize(frase)\n",
    "    token_words\n",
    "    stem_sentence = []\n",
    "    \n",
    "    spanishStemmer = SnowballStemmer(\"spanish\")\n",
    "    \n",
    "    for word in token_words:\n",
    "        stem_sentence.append(spanishStemmer.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "una fras de prueb en castellan par aplic stemming \n"
     ]
    }
   ],
   "source": [
    "frase1 = \"Una frase de prueba en castellano para aplicar stemming\"\n",
    "\n",
    "x = stemfraseesp(frase1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Es el proceso de agrupar las distintas formas de una palabra para que puedan analizarse como un solo elemento, identificado por el lema de la palabra o forma de diccionario.\n",
    "\n",
    "A diferencia de stemming, la lematización depende de la identificación correcta de la parte deseada del discurso y el significado de una palabra en una oración, así como dentro del contexto más amplio que rodea esa oración, como las oraciones vecinas o incluso un documento completo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wordnet: https://wordnet.princeton.edu/ - https://www.nltk.org/api/nltk.corpus.reader.html#module-nltk.corpus.reader.wordnet\n",
    "    \n",
    "udhr  https://www.nltk.org/api/nltk.corpus.reader.html#module-nltk.corpus.reader.udhr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mbeati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package udhr to\n",
      "[nltk_data]     C:\\Users\\mbeati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package udhr is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('udhr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations = \"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Universal',\n",
       " 'Declaration',\n",
       " 'of',\n",
       " 'Human',\n",
       " 'Rights',\n",
       " 'Preamble',\n",
       " 'Whereas',\n",
       " 'recognition',\n",
       " 'of',\n",
       " 'the',\n",
       " 'inherent',\n",
       " 'dignity',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'equal',\n",
       " 'and',\n",
       " 'inalienable',\n",
       " 'rights',\n",
       " 'of']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udhr = nltk.corpus.udhr.words('English-Latin1')\n",
    "udhr[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "Universal           Universal           \n",
      "Declaration         Declaration         \n",
      "of                  of                  \n",
      "Human               Human               \n",
      "Rights              Rights              \n",
      "Preamble            Preamble            \n",
      "Whereas             Whereas             \n",
      "recognition         recognition         \n",
      "of                  of                  \n",
      "the                 the                 \n",
      "inherent            inherent            \n",
      "dignity             dignity             \n",
      "and                 and                 \n",
      "of                  of                  \n",
      "the                 the                 \n",
      "equal               equal               \n",
      "and                 and                 \n",
      "inalienable         inalienable         \n",
      "rights              right               \n",
      "of                  of                  \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence_words = udhr[:20] \n",
    "\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
